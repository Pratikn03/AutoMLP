{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c557546",
   "metadata": {},
   "source": [
    "# Set Up Notebook Environment\n",
    "This notebook shows how to trigger the end-to-end AutoML pipeline with a tighter time budget and document the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "REPO_ROOT = Path.cwd()\n",
    "os.environ.setdefault(\"FLAML_TIME_BUDGET\", \"30\")  # Keep AutoML search capped at ~30 seconds per dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958e4f6",
   "metadata": {},
   "source": [
    "# Demonstrate Code with Inline Comments\n",
    "The cell below kicks off the orchestrator script and captures its return details for quick inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318bcfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cmd = [\"python\", \"scripts/run_all.py\"]  # Run the consolidated training + analysis pipeline\n",
    "runtime_env = os.environ.copy()\n",
    "runtime_env[\"PYTHONPATH\"] = str(REPO_ROOT)  # Ensure relative imports resolve\n",
    "\n",
    "result = subprocess.run(\n",
    "    pipeline_cmd,\n",
    "    cwd=REPO_ROOT,\n",
    "    env=runtime_env,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    check=False,\n",
    ")\n",
    "\n",
    "print(f\"Return code: {result.returncode}\")\n",
    "print(result.stdout[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49c737",
   "metadata": {},
   "source": [
    "# Pair Code Cells with Markdown Explanations\n",
    "The previous cell invokes `scripts/run_all.py` using the repository root as the working directory. The environment clone inherits the 30-second FLAML cap, so each dataset should finish faster. We capture stdout and the return code to verify whether the orchestrator succeeded without scrolling through the entire log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c285a3",
   "metadata": {},
   "source": [
    "# Run and Verify Output\n",
    "This final step inspects the leaderboard artifacts produced by the pipeline so we can confirm that fresh metrics were persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_paths = [\n",
    "    REPO_ROOT / \"reports\" / \"leaderboard_multi.csv\",\n",
    "    REPO_ROOT / \"reports\" / \"leaderboard.csv\",\n",
    "]\n",
    "for run_board in sorted((REPO_ROOT / \"runs\").glob(\"*/reports/leaderboard.csv\")):\n",
    "    candidate_paths.append(run_board)\n",
    "\n",
    "for path in candidate_paths:\n",
    "    if path.exists():\n",
    "        chosen_leaderboard = path\n",
    "        break\n",
    "else:  # pragma: no cover - defensive guard for missing artifacts\n",
    "    raise FileNotFoundError(\"No leaderboard CSV was generated. Make sure the orchestrator completed.\")\n",
    "\n",
    "print(f\"Inspecting leaderboard at: {chosen_leaderboard.relative_to(REPO_ROOT)}\")\n",
    "df = pd.read_csv(chosen_leaderboard)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a1eb1",
   "metadata": {},
   "source": [
    "# Run Additional Python Entry Points\n",
    "If you need to call other scripts in this repository, reuse the helper below. It keeps the working directory anchored at the repository root and propagates the same environment (including `PYTHONPATH` and the 30-second FLAML budget)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_python(relative_path: str, *extra_args: str) -> subprocess.CompletedProcess:\n",
    "    \"\"\"Execute a repository Python script with consistent environment settings.\"\"\"\n",
    "    command = [\"python\", relative_path, *extra_args]\n",
    "    print(f\"Running: {' '.join(command)}\")\n",
    "    completed = subprocess.run(\n",
    "        command,\n",
    "        cwd=REPO_ROOT,\n",
    "        env=runtime_env,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False,\n",
    "    )\n",
    "    print(f\" â†’ exit code {completed.returncode}\")\n",
    "    if completed.stdout:\n",
    "        print(completed.stdout[-500:])\n",
    "    if completed.stderr:\n",
    "        print(\"stderr:\\n\" + completed.stderr[-500:])\n",
    "    return completed\n",
    "\n",
    "# Example: uncomment to run individual components with the same settings\n",
    "# run_python(\"scripts/run_automl_suite.py\")\n",
    "# run_python(\"Project/analysis/explain_shap.py\")\n",
    "# run_python(\"Project/trainers/train_boosters.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d3ad9",
   "metadata": {},
   "source": [
    "# Summarize Leaderboard Metrics\n",
    "To quickly compare frameworks, the next cell aggregates the latest leaderboard by computing mean and standard deviation for the key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd22fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_track = [col for col in [\"f1_macro\", \"accuracy\", \"roc_auc_ovr\", \"avg_precision_ovr\"] if col in df.columns]\n",
    "if not metrics_to_track:\n",
    "    raise ValueError(\"No common metric columns found in the leaderboard.\")\n",
    "\n",
    "summary = (\n",
    "    df.groupby(\"framework\")[metrics_to_track]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .round(4)\n",
    ")\n",
    "summary.columns = [\"_\".join(filter(None, level)).strip(\"_\") for level in summary.columns]\n",
    "display(summary)\n",
    "\n",
    "if \"dataset\" in df.columns:\n",
    "    dataset_rollup = (\n",
    "        df.groupby([\"dataset\", \"framework\"])[metrics_to_track]\n",
    "        .agg([\"mean\", \"std\"])\n",
    "        .round(4)\n",
    "    )\n",
    "    dataset_rollup.columns = [\"_\".join(filter(None, level)).strip(\"_\") for level in dataset_rollup.columns]\n",
    "    display(dataset_rollup)\n",
    "else:\n",
    "    print(\"Dataset column not present; per-framework summary shown above.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
